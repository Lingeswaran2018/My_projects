# -*- coding: utf-8 -*-
"""potato-disease-classification-model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MQYw3K5BVnVPhhyItpoCGNQ7CilvtaZb

# Potato Disease Classification

Dataset credits: https://www.kaggle.com/arjuntejaswi/plant-village

### Import all the Dependencies
"""

import tensorflow as tf
from tensorflow.keras import models, layers
import matplotlib.pyplot as plt
from IPython.display import HTML
import numpy as np

from google.colab import drive # Mounting Google Drive to the Colab notebook
drive.mount('/content/drive')

# Importing the os module for interacting with the operating system
import os

# Changing the current directory to the specified path where the Google Drive is mounted
os.chdir('/content/drive/MyDrive/Colab Notebooks/potato_disease')

# Defining the path to the zip file
zip_path = '/content/drive/MyDrive/Colab Notebooks/potato_disease/data set_potato.zip'

# Checking if the zip file exists
if os.path.exists(zip_path):
    print("File exists.")  # If the zip file exists, print a message indicating its existence
else:
    print("File does not exist.")  # If the zip file does not exist, print a message indicating its non-existence

import zipfile

# Specify the path to the zip file
zip_path = 'data set_potato.zip' # Update this to your zip file's name

# Specify the directory where you want to extract the zip file
extract_path = '/content/extracted_files' # You can change this to your preferred directory

# Create the directory if it doesn't exist
if not os.path.exists(extract_path):
    os.makedirs(extract_path)

# Extract the zip file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

"""### Set all the Constants"""

BATCH_SIZE = 32
IMAGE_SIZE = 256
CHANNELS=3
EPOCHS=20

"""### Import data into tensorflow dataset object

We will use image_dataset_from_directory api to load all images in tensorflow dataset: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory
"""

DATA_SET_DIR="/content/extracted_files/data set_potato"
dataset = tf.keras.preprocessing.image_dataset_from_directory(
    DATA_SET_DIR,
    seed=123,
    shuffle=True,
    image_size=(IMAGE_SIZE,IMAGE_SIZE),
    batch_size=BATCH_SIZE
)

"""**Watch below video on tensorflow input pipeline first if you don't know about tensorflow datasets**"""

class_names = dataset.class_names
class_names

len(dataset)   #32 batch size

# Iterate over the first batch of images and labels from the dataset
for image_batch, labels_batch in dataset.take(1):
    # Print the shape of the image batch tensor
    print("Shape of image batch tensor:", image_batch.shape)

    # Print the labels batch tensor as a NumPy array
    print("Labels batch (NumPy array):", labels_batch.numpy())

    # Print the shape of the first image in the batch
    print("Shape of the first image in the batch:", image_batch[0].shape)

    # Print the pixel values of the first image in the batch as a NumPy array
    print("Pixel values of the first image (NumPy array):", image_batch[0].numpy())

    # Print the first image in the batch
    print("First image in the batch:")
    print(image_batch[0])

# Iterate over the first batch of images and labels from the dataset
for image_batch, label_batch in dataset.take(1):
    # Display the first image in the batch using Matplotlib
    plt.imshow(image_batch[0].numpy().astype("uint8"))

# Show the plot with the first image in the batch
plt.show()

"""As you can see above, each element in the dataset is a tuple. First element is a batch of 32 elements of images. Second element is a batch of 32 elements of class labels

### Visualize some of the images from our dataset
"""

# Set the size of the figure to 10x10 inches
plt.figure(figsize=(10, 10))

# Iterate over the first batch of images and labels from the dataset
for image_batch, labels_batch in dataset.take(1):
    # Iterate over each image in the batch (maximum 12)
    for i in range(min(len(image_batch), 12)):
        # Create a subplot in a 3x4 grid, with index incremented by 1
        ax = plt.subplot(3, 4, i + 1)

        # Display the image using imshow()
        plt.imshow(image_batch[i].numpy().astype("uint8"))

        # Set the title of the subplot to the corresponding class name of the image
        plt.title(class_names[labels_batch[i]])

        # Remove axis ticks and labels from the subplot for cleaner visualization
        plt.axis("off")

# Show the plot with images and captions
plt.show()

"""### Function to Split Dataset

Dataset should be bifurcated into 3 subsets, namely:
1. Training: Dataset to be used while training
2. Validation: Dataset to be tested against while training
3. Test: Dataset to be tested against after we trained a model
"""

len(dataset)

train_size = 0.8
len(dataset)*train_size

train_ds = dataset.take(102)
len(train_ds)

test_ds_and_validation = dataset.skip(102)
len(test_ds_and_validation)

val_size=0.1
len(dataset)*val_size

val_ds = test_ds_and_validation.take(12)
len(val_ds)

test_ds = test_ds_and_validation.skip(12)
len(test_ds)

"""train_ds=102,
val_ds=12,
test_ds=14,

"""

def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):
    # Ensure that the sum of train, validation, and test splits equals 1
    assert (train_split + test_split + val_split) == 1

    # Get the total size of the dataset
    ds_size = len(ds)

    # Shuffle the dataset if required
    if shuffle:
        ds = ds.shuffle(shuffle_size, seed=12)

    # Calculate the sizes of training, validation, and test sets
    train_size = int(train_split * ds_size)
    val_size = int(val_split * ds_size)

    # Split the dataset into training, validation, and test sets
    train_ds = ds.take(train_size)  # Take the first 'train_size' elements for training
    val_ds = ds.skip(train_size).take(val_size)  # Skip 'train_size' elements and take 'val_size' elements for validation
    test_ds = ds.skip(train_size).skip(val_size)  # Skip 'train_size' and 'val_size' elements for testing

    return train_ds, val_ds, test_ds  # Return the training, validation, and test datasets

train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)

len(train_ds)

len(val_ds)

len(test_ds)

"""### Cache, Shuffle, and Prefetch the Dataset"""

# Cache, shuffle, and prefetch the training dataset
train_ds = train_ds.cache()  # Cache the elements of the training dataset for faster access
train_ds = train_ds.shuffle(1000)  # Shuffle the training dataset with a buffer size of 1000
train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)  # Prefetch the next batch of elements asynchronously for better performance

# Cache, shuffle, and prefetch the validation dataset
val_ds = val_ds.cache()  # Cache the elements of the validation dataset for faster access
val_ds = val_ds.shuffle(1000)  # Shuffle the validation dataset with a buffer size of 1000
val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)  # Prefetch the next batch of elements asynchronously for better performance

# Cache, shuffle, and prefetch the test dataset
test_ds = test_ds.cache()  # Cache the elements of the test dataset for faster access
test_ds = test_ds.shuffle(1000)  # Shuffle the test dataset with a buffer size of 1000
test_ds = test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)  # Prefetch the next batch of elements asynchronously for better performance

"""## Building the Model

### Creating a Layer for Resizing and Normalization
Before we feed our images to network, we should be resizing it to the desired size.
Moreover, to improve model performance, we should normalize the image pixel value (keeping them in range 0 and 1 by dividing by 256).
This should happen while training as well as inference. Hence we can add that as a layer in our Sequential Model.

You might be thinking why do we need to resize (256,256) image to again (256,256). You are right we don't need to but this will be useful when we are done with the training and start using the model for predictions. At that time somone can supply an image that is not (256,256) and this layer will resize it
"""

# Create a sequential model to resize and rescale images
resize_and_rescale = tf.keras.Sequential([
    # Resize images to the specified dimensions using Resizing layer
    layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),
    # Rescale pixel values to range [0, 1] using Rescaling layer
    layers.experimental.preprocessing.Rescaling(1./255),
])

"""### Data Augmentation
Data Augmentation is needed when we have less data, this boosts the accuracy of our model by augmenting the data.
"""

# Create a sequential model for data augmentation
data_augmentation = tf.keras.Sequential([
    # Randomly flip images horizontally and vertically
    layers.experimental.preprocessing.RandomFlip("horizontal_and_vertical"),
    # Randomly rotate images by a fraction of 0.2
    layers.experimental.preprocessing.RandomRotation(0.2),
])

"""#### Applying Data Augmentation to Train Dataset"""

# Apply data augmentation to the training dataset using the map function
train_ds = train_ds.map(
    lambda x, y: (data_augmentation(x, training=True), y)
)

# Prefetch the preprocessed training dataset for better performance
train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)

"""**Watch below video if you are not familiar with data augmentation**

### Model Architecture
We use a CNN coupled with a Softmax activation in the output layer. We also add the initial layers for resizing, normalization and Data Augmentation.

**We are going to use convolutional neural network (CNN) here. CNN is popular for image classification tasks. Watch below video to understand fundamentals of CNN**
"""

# Define the input shape for the model
input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)

# Define the number of classes
n_classes = 3

# Create a sequential model
model = models.Sequential([
    # Preprocessing layers to resize and rescale images
    resize_and_rescale,

    # Convolutional layers with ReLU activation and max pooling
    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    # Flatten layer to convert 2D feature maps to a 1D vector
    layers.Flatten(),

    # Fully connected dense layers with ReLU activation
    layers.Dense(64, activation='relu'),

    # Output layer with softmax activation for classification
    layers.Dense(n_classes, activation='softmax'),
])

# Build the model with the specified input shape
model.build(input_shape=input_shape)

model.summary()

"""### Compiling the Model
We use `adam` Optimizer, `SparseCategoricalCrossentropy` for losses, `accuracy` as a metric
"""

model.compile(
    optimizer='adam',  # Using the Adam optimizer for training, which adapts the learning rate during training
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),  # Defining the loss function as Sparse Categorical Crossentropy, suitable for multi-class classification problems with integer labels
    metrics=['accuracy']  # Monitoring model performance during training using accuracy metric
)

history = model.fit(
    train_ds,  # Training dataset
    batch_size=BATCH_SIZE,  # Batch size for training
    validation_data=val_ds,  # Validation dataset
    verbose=1,  # Verbosity mode (1: progress bar, 0: silent)
    epochs=EPOCHS,  # Number of epochs for training
)

scores = model.evaluate(test_ds)

"""**You can see above that we get 100.00% accuracy for our test dataset. This is considered to be a pretty good accuracy**"""

scores

"""Scores is just a list containing loss and accuracy value

### Plotting the Accuracy and Loss Curves
"""

history

"""You can read documentation on history object here: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History"""

history.params

history.history.keys()

"""**loss, accuracy, val loss etc are a python list containing values of loss, accuracy etc at the end of each epoch**"""

type(history.history['loss'])

len(history.history['loss'])

history.history['loss'][:5] # show loss for first 5 epochs

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(range(EPOCHS), acc, label='Training Accuracy')
plt.plot(range(EPOCHS), val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(range(EPOCHS), loss, label='Training Loss')
plt.plot(range(EPOCHS), val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

"""### Run prediction on a sample image"""

import numpy as np

# Iterate over the first batch of images and labels in the test dataset
for images_batch, labels_batch in test_ds.take(1):

    # Convert the first image in the batch to numpy array and change data type to 'uint8'
    first_image = images_batch[0].numpy().astype('uint8')

    # Extract the label of the first image
    first_label = labels_batch[0].numpy()

    # Print information about the first image
    print("First image to predict:")
    plt.imshow(first_image)  # Display the first image
    plt.show()
    print("Actual label:", class_names[first_label])  # Print the actual label of the first image

    # Make predictions for the entire batch of images
    batch_prediction = model.predict(images_batch)

    # Print the predicted label of the first image
    print("Predicted label:", class_names[np.argmax(batch_prediction[0])])

"""### Write a function for inference"""

def predict(model, img):
    # Convert the input image to a numpy array
    img_array = tf.keras.preprocessing.image.img_to_array(img)

    # Expand the dimensions of the image array to match the expected input shape of the model
    img_array = tf.expand_dims(img_array, 0)

    # Make predictions using the model
    predictions = model.predict(img_array)

    # Get the predicted class (the class with the highest probability) and its confidence level
    predicted_class = class_names[np.argmax(predictions[0])]  # Extract the predicted class label
    confidence = round(100 * np.max(predictions[0]), 2)  # Calculate the confidence level (%)

    # Return the predicted class and confidence level
    return predicted_class, confidence

"""**Now run inference on few sample images**"""

plt.figure(figsize=(15, 15))  # Set the figure size for the plot

# Iterate over the first batch of images and labels in the test dataset
for images, labels in test_ds.take(1):
    # Iterate over a subset of images (e.g., first 9 images)
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)  # Create a subplot for each image
        plt.imshow(images[i].numpy().astype("uint8"))  # Display the image

        # Make predictions for the current image using the predict function
        predicted_class, confidence = predict(model, images[i].numpy())

        # Get the actual class label from the dataset
        actual_class = class_names[labels[i]]

        # Set the title of the subplot with actual and predicted labels and confidence level
        plt.title(f"Actual: {actual_class},\n Predicted: {predicted_class}.\n Confidence: {confidence}%")

        plt.axis("off")  # Turn off axis for the subplot

plt.show()  # Display the plot

"""### Saving the Model
We append the model to the list of models as a new version
"""

# Assuming 'model' is your trained model

# Specify the path to the folder in your Google Drive where you want to save the model
save_path = '/content/models'

# Create the directory if it doesn't exist
import os

if not os.path.exists(save_path):
    os.makedirs(save_path)
model_version = max([int(i) for i in os.listdir(save_path) + [0]]) + 1

# Save the model
model.save(f"{save_path}/model_{model_version}.h5")